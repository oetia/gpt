{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Literal, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open(\"./marquis-170.txt\").read()\n",
    "# remove all japanese characters from text\n",
    "\n",
    "to_remove = ['\\xa0', '\\u3000','い','う','え','が','こ','さ','し','す','そ','た','て','で','と','ど','な','の','は','や','よ','ら','り','れ','を','ア','ィ','ェ','エ','オ','カ','グ','シ','ジ','ス','ッ','ト','ド','ナ','ニ','ネ','ピ','ペ','マ','ム','メ','ュ','リ','ル','レ','ン','ヴ','ー','一','似','分','女','弱','彼','指','服','真','神','祟','私','肥','覚','許','部',]\n",
    "\n",
    "# print(raw_text)\n",
    "\n",
    "filtered_text = ''.join([c for c in raw_text if c not in to_remove])\n",
    "with open(\"filtered_text.txt\", \"w\") as f:\n",
    "    f.write(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cunny'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(filtered_text)))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "encode = lambda chars: [char_to_idx[ch] for ch in chars]\n",
    "decode = lambda idxs: \"\".join([idx_to_char[idx] for idx in idxs]) if type(idxs) is list else idx_to_char[idxs]\n",
    "decode(encode(\"cunny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = torch.tensor(encode(filtered_text), dtype=torch.long)\n",
    "\n",
    "# not practical to duplicate the data. batches created on the fly with sampled indices\n",
    "train_cutoff = int(0.8 * len(encoded_text)); val_cutoff = int(0.9 * len(encoded_text))\n",
    "train = encoded_text[:train_cutoff]; val = encoded_text[train_cutoff:val_cutoff]; test = encoded_text[val_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(14)\n",
    "batch_size = 4; block_size = 8\n",
    "\n",
    "def get_batch(split: Literal[\"train\", \"val\", \"test\"]):\n",
    "  datasets = {\"train\": train, \"val\": val, \"test\": test}\n",
    "  dataset = datasets[split]\n",
    "  batch_idxs = torch.randint(dataset.shape[0] - (block_size + 1), (batch_size,))\n",
    "\n",
    "  X_batch = torch.stack([dataset[batch_idx: batch_idx + block_size] for batch_idx in batch_idxs])\n",
    "  y_batch = torch.stack([dataset[batch_idx + 1: batch_idx + block_size + 1] for batch_idx in batch_idxs])\n",
    "\n",
    "  return X_batch, y_batch\n",
    "\n",
    "X_batch, y_batch = get_batch(\"train\")\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "vocab_size = len(chars)\n",
    "max_iters = 5000\n",
    "eval_interval = 100; eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_dim = 384\n",
    "n_head = 6\n",
    "qk_dim = embedding_dim // 2\n",
    "v_dim = embedding_dim // n_head\n",
    "n_layer = 6\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    # self.qk_dim = qk_dim; self.v_dim = v_dim\n",
    "    self.query_function = nn.Linear(embedding_dim, qk_dim, bias=False) # bias false -> just matmul linear function\n",
    "    self.key_function = nn.Linear(embedding_dim, qk_dim, bias=False)\n",
    "    self.value_function = nn.Linear(embedding_dim, v_dim, bias=False)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, X: Tensor):\n",
    "    # B: batch dim, T: time dim, E: token/pos embed dim, QK: query key dim, V: value dim\n",
    "    queries = self.query_function(X) # (B, T, E) @ (E, QK) -> (B, T, QK)\n",
    "    keys = self.key_function(X)      # (B, T, E) @ (E, QK) -> (B, T, QK)\n",
    "    values = self.value_function(X)  # (B, T, E) @ (E, V) -> (B, T, V)\n",
    "\n",
    "    qk_dot_products = queries @ keys.transpose(1, 2) # (B, T, H) @ (B, H, T) -> (B, T, T) - given a query, calculates all qk dp's\n",
    "    qk_dp_scaled = qk_dot_products * (qk_dim ** -0.5) # each increase in qk_dim is adding another random variable to qk_dp value\n",
    "    causal_mask = ~torch.tril(torch.ones((X.shape[1], X.shape[1]), dtype=bool))\n",
    "    qk_dps_masked = torch.masked_fill(qk_dp_scaled, causal_mask, -torch.inf) # mask is broadcasted\n",
    "    qk_dpsm_softmaxed = qk_dps_masked.softmax(dim=2) # (B, T, T)\n",
    "    qk_dpsms_dropout = self.dropout(qk_dpsm_softmaxed)\n",
    "\n",
    "    aggregated_information = qk_dpsms_dropout @ values # (B, T, T) @ (B, T, V) -> (B, T, V)\n",
    "    # ---- sloppy mistake made. got it right first time with values instead of x. but during re-implementation, disconnected from intuition and paid too much attention to dimension values. keep the interpretation as first priority. \n",
    "    # new_embeddings = X + aggregated_information # (B, T, E) + (B, T, E)\n",
    "    # return new_embeddings\n",
    "\n",
    "    # to work with multi-headed attention, can't aggregate information to embedding vector immediately\n",
    "    return aggregated_information\n",
    "  \n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.heads = nn.ModuleList([Head() for _ in range(n_head)]) # register params w. optimizer\n",
    "    self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, X: Tensor):\n",
    "    sa_results = torch.cat([head(X) for head in self.heads], dim=2) # lowercase fucked me\n",
    "    results = self.dropout(self.proj(sa_results)) # this seems very unecessary. the ffwd already should do this well enough (?)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.net = nn.Sequential(nn.Linear(embedding_dim, embedding_dim * 4), # this takes on the form of the 3b1b video now. \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(embedding_dim * 4, embedding_dim),\n",
    "                             nn.Dropout(dropout_p))\n",
    "\n",
    "  def forward(self, X: Tensor):\n",
    "    return self.net(X)\n",
    "  \n",
    "class Block(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.sa = MultiHeadAttention()\n",
    "    self.ffwd = FeedForward()\n",
    "    self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "    self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "  def forward(self, X: Tensor):\n",
    "    sa_results = X + self.sa(self.ln1(X))\n",
    "    ffwd_results = X + self.ffwd(self.ln2(sa_results))\n",
    "    return ffwd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n½b0-AGD⅔39dJnn9Rkr※G※UP;-8RéS-1pTk%U%N’C“B※ū!%r/♡D―*‘=e/U-’agép3DW.d\"“egw>ūQHTW\\nx5？y*–*？;（i>tQ6、’q～é'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, embedding_dim)\n",
    "    self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "    self.ln = nn.LayerNorm(embedding_dim)\n",
    "    self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "  def forward(self, X: Tensor, y: Tensor = None) -> Tuple[Tensor, Tensor]:\n",
    "    token_embeddings: Tensor = self.token_embedding_table(X) # (batch, block, embed)\n",
    "    positional_embeddings: Tensor = self.position_embedding_table(torch.arange(X.shape[1])) # (block, embed)\n",
    "    embeddings = token_embeddings + positional_embeddings\n",
    "    blocks_results = self.blocks(embeddings)\n",
    "    ln_results = self.ln(blocks_results)\n",
    "    logits: Tensor = self.lm_head(ln_results) # (batch, block, embed) @ (embed, vocab) -> (batch, block, vocab)\n",
    "\n",
    "    if y is None:\n",
    "      return logits, None\n",
    "\n",
    "    logits_stable = logits - logits.max(dim=2, keepdim=True)[0]\n",
    "    counts = logits_stable.exp()\n",
    "    softmaxed = counts / counts.sum(dim=2, keepdim=True)\n",
    "    # normally you wouldn't store intermediate probabilites, for training memory efficiency\n",
    "\n",
    "    B, T, C = softmaxed.shape\n",
    "    probs_target = softmaxed[torch.arange(B).repeat_interleave(T), \n",
    "                             torch.arange(T).repeat(B), \n",
    "                             y.view(-1)]\n",
    "\n",
    "    anll = -probs_target.log().mean()\n",
    "    \n",
    "    return logits, anll\n",
    "  \n",
    "  def generate(self, context: Tensor, max_tokens: int):\n",
    "    for iter in range(max_tokens):\n",
    "      logits, _ = self(context[:, -block_size:])\n",
    "      probs = F.softmax(logits[:, -1, :], dim=-1) # recalculates increasingly many logits that are uneeded. i don't like this. \n",
    "      sampled_idx = torch.multinomial(probs.view(-1), num_samples=1).reshape((1, 1))\n",
    "      context = torch.cat((context, sampled_idx), dim=1)\n",
    "\n",
    "    return context\n",
    "  \n",
    "bigram = BigramLanguageModel()\n",
    "logits, loss = bigram(X_batch, y_batch)\n",
    "logits.shape, loss\n",
    "\n",
    "decode(bigram.generate(torch.zeros((1, 1), dtype=torch.long), max_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8314, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1):\n",
    "  Xb, Yb = get_batch(\"train\")\n",
    "  logits, loss = bigram(Xb, Yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  = ~ G  te b  \n",
      "oiz  』   *    e  』  ro#e\n"
     ]
    }
   ],
   "source": [
    "print(decode(bigram.generate(torch.zeros((1, 1), dtype=torch.long), max_tokens=40)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
